[transmogrifier]
pipeline =
# Initialize the pipeline
    source
    root

# Beginning of the crawling/spidering recursion
    breaker
    start

# Extract content data from parents
    url
    parent-path

# Drop items that shouldn't be crawled/spidered
    log-drop
    drop
    log-crawl

# Retrieve responses from a local cache first, then by requesting a URL
    cache
    log-request
    response
    log-cache
    headers
    cache-headers
    file

# Parse XML trees for heirarchy/structure and for links to crawl/spider
    parse
    content
    walk-content
    content-element
    content-parent
    nav
    walk-nav
    nav-parent

# Extract content data from crawled/spidered links for prior to retrieval
    href
    element-title
    cleanup-crawl

# Send newly crawled/spidered links back to the top, recurse
    crawl

# Extract content information from parsed XML
    remoteUrl
    title
    description
    subject
    text
    cleanup-trees

# Extract content information from already extracted information
    id
    path
    type
    modificationDate
    defaultpage
    folders
    excludeFromNav

# Add the content
    debug
    drop-construct
    log-content
    construct
    savepoint

## Initialize the pipeline with a URL to crawl

#    Start the pipeline with a single CSV row
[source]
blueprint = collective.transmogrifier.sections.csvsource
filename = collective.transmogrifier.sections:xmlwalker.csv

#    Set the site root URL to start crawling from
[root]
blueprint = collective.transmogrifier.sections.inserter
key = string:_url
condition = python:item.get('_path') == '/'
url = ${root:scheme}://${root:netloc}${root:path}
value = python:modules['urlparse'].urlsplit('${root:url}')


## Central crawling loop
#    This is the source for items from links that are crawled
[start]
blueprint = collective.transmogrifier.sections.listsource


## Extract content data from parents
# Any parent information that depends on the parent crawling response
# and is needed by it's children, such as paths, must be extracted
# after the walked items to be crawled get sent back to the list
# source because the crawled items must be exhausted before the list
# source will emit the parents

[url]
blueprint = collective.transmogrifier.sections.inserter
key = string:_url
condition = python:'_href' in item and item.setdefault(\
    '_url', modules['urlparse'].urlsplit(item['_href']))
value = python:modules['urlparse'].SplitResult(\
    item['_url'].scheme or '${root:scheme}',\
    item['_url'].netloc or '${root:netloc}',\
    modules['urllib'].quote(modules['posixpath'].abspath(\
        modules['posixpath'].join(modules['posixpath'].dirname(\
            getattr(item.get('_parent', {}).get('_url'), 'path',\
                '${root:path}')), item['_url'].path))), '', '')

[parent-path]
blueprint = collective.transmogrifier.sections.inserter
key = string:_parent_path
condition = exists:item/_parent/_path
# Also removes _parent
value = python:item.update(_parent_path=(item['_parent']['_path']))\
    or item.pop('_parent', True) and item['_parent_path']


## Drop items not to crawl

[log-drop]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = not:${drop:condition}
delete = ${log-content:delete}

[drop]
blueprint = collective.transmogrifier.sections.condition
ignored-extensions =
condition = python:'_url' not in item or (\
# Drop external links in content, let links in nav become Link content
    (modules['urlparse'].urlunsplit(item['_url']).startswith(\
        '${root:url}') or '_content_element' not in item)\
# skip certain file types
    and modules['os.path'].splitext(\
        item['_url'].path or '_.html') not in [\
        ext.strip() for ext in """${drop:ignored-extensions}""".split()\
        if ext.strip()]\
# Don't re-process previously processed links
    and ('_content_element' not in item\
         or modules['urlparse'].urlunsplit(item['_url'])\
         not in transmogrifier.__annotations__.get('xmlwalker.paths', {})))

[log-crawl]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = ${crawl:condition}
key = _url


## Get the response for the URL

[cache]
blueprint = collective.transmogrifier.sections.inserter
key = string:_cache
condition = python:'_url' in item
value = python:not modules['os.path'].exists(modules['os.path'].dirname(\
    item.setdefault('_cache', modules['os.path'].join(\
        'var', 'xmlwalker', item['_url'].scheme, item['_url'].netloc,\
        modules['os.path'].dirname(item['_url'].path.lstrip('/')),\
        modules['os.path'].basename(item['_url'].path.lstrip('/'))\
        or 'index.html',)))) and modules['os'].makedirs(\
            modules['os.path'].dirname(item['_cache'])) or item['_cache']

[log-request]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = ${response:condition}
key = _url

[response]
blueprint = collective.transmogrifier.sections.inserter
key = string:_response
condition-python = '_url' in item\
    and not modules['os.path'].exists(item.get('_cache', ''))
condition = python:${response:condition-python}
value = python:modules['urllib2'].urlopen(\
    modules['urlparse'].urlunsplit(item['_url']))

[log-cache]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = python:'_cache' not in item and ${response:condition-python}
key = _cache

# Get response headers from cache if available or populate the cache
[headers]
blueprint = collective.transmogrifier.sections.inserter
key = string:_headers
condition = python:'_cache' in item and '_response' in item and (\
    modules['ConfigParser'].SafeConfigParser(dict(\
        item['_response'].info(), url=item['_response'].geturl())).write(\
            open(item['_cache'] + '.metadata', 'w')) or True)
value = python:dict(item['_response'].info())

[cache-headers]
blueprint = collective.transmogrifier.sections.inserter
key = string:_headers
condition = python:'_headers' not in item and '_cache' in item\
    and (item.update(_headers=modules['ConfigParser'].SafeConfigParser())\
         or item['_headers'].read(item['_cache'] + '.metadata') and True)
value = python:dict(item['_headers'].defaults())

[file]
blueprint = collective.transmogrifier.sections.inserter
key = string:file
# Also writes the cache file
condition = python:'_cache' in item and ('_response' in item and\
    open(item['_cache'], 'w').writelines(item['_response']) or True)
value = python:open(item['_cache'])

[parse]
blueprint = collective.transmogrifier.sections.inserter
key = string:_tree
condition = python:item.get('_headers') and item.get('_cache')\
    and item['_headers'].get('content-type', '').startswith('text/html')
value = python:modules['lxml.html'].parse(item['_cache'])


## Get links to crawl from content body next

[content]
blueprint = collective.transmogrifier.sections.inserter
key = string:_content
condition = python:'_tree' in item
# Customize this xpath expression to isolate the content body elements
xpath = //*[@id='content' or contains(@class, 'content')]/*
value = python:item['_tree'].xpath("${content:xpath}")

[walk-content]
blueprint = collective.transmogrifier.sections.xmlwalker
trees-key = _content
key = string:_content_element
type-key = nothing
default-page-key = nothing

[content-element]
blueprint = collective.transmogrifier.sections.inserter
key = string:_element
condition = python:'_content_element' in item and not 'file' in item
value = python:item.update(_element=item['_content_element'])\
    or item.update(_content_element=len(item['_content_element']))\
    or item['_element']

# Ignore content heirarchy, all relative to original content
[content-parent]
blueprint = collective.transmogrifier.sections.inserter
key = string:_parent
# Apply only to items walked from the content
condition = python:'_parent' in item and '_content_element' in item
value = ${nav-parent:value}


## Get links to crawl from site nav first

[nav]
blueprint = collective.transmogrifier.sections.inserter
key = string:_nav
condition = python:'_tree' in item
# Customize this xpath expression to isolate the navigation elements
xpath = //*[contains(@class, 'navbar')]//ul[contains(@class, 'nav')]
value = python:item['_tree'].xpath("${nav:xpath}")

[walk-nav]
blueprint = collective.transmogrifier.sections.xmlwalker
trees-key = _nav

# Heirarchy from the navbar should relative to the root
[nav-parent]
blueprint = collective.transmogrifier.sections.inserter
key = string:_parent
# Apply only to items walked from the navbar
condition = python:'_parent' in item\
    and '_content_element' not in item and '_has_nav' in item['_parent']
value = python:[item.update(_parent=item['_parent']['_parent'])\
    for idx in modules['itertools'].takewhile(\
        lambda idx, item=item: '_parent' in item.get('_parent', {}),\
        modules['itertools'].count())][:0] or item['_parent']


## Get everything we need from memory intensive parent/child references

[href]
blueprint = collective.transmogrifier.sections.inserter
key = string:_href
xpath = (@href | @src)[.!='' and not(starts-with(., '#'))]
condition = python:'_element' in item\
    and item['_element'].xpath("${href:xpath}")
value = python:' '.join(item['_element'].xpath("${href:xpath}")).strip()

[element-title]
blueprint = collective.transmogrifier.sections.inserter
key = string:title
condition = python:not item.get('title') and '_element' in item
value = python:unicode(item['_element'].text_content().strip()\
                       or item['_element'].attrib.get('alt', '').strip())


## Free/remove memory intensive references
[cleanup-crawl]
blueprint = collective.transmogrifier.sections.inserter
key = string:_element
condition = python:item.update(('_has' + key, len(item.pop(key))) for key in (\
        '_element', '_defaultpage') if key in item)
value = nothing


## Send child items back to the top of the crawl loop
[crawl]
blueprint = collective.transmogrifier.sections.listappender
condition = python:'file' not in item and '_href' in item
section = start


## Extract content data from XML trees

[remoteUrl]
blueprint = collective.transmogrifier.sections.inserter
key = string:remoteUrl
# Use Links for nav elements to external URLs or already crawled internal URLs
condition-python = '_has_element' in item\
    and '_content_element' not in item and '_url' in item\
    and modules['urlparse'].urlunsplit(item['_url']) in\
        transmogrifier.__annotations__.get('xmlwalker.paths', {})
condition = python:item.get('_type', 'Link') == 'Link'\
    and ${remoteUrl:condition-python}
value = python:(modules['urlparse'].urlunsplit(\
    item['_url']).startswith("${root:url}") and item['_url'].path)\
    or modules['urlparse'].urlunsplit(item['_url'])

[title]
blueprint = collective.transmogrifier.sections.inserter
# Customise xpath to change title extraction
xpath = /html/head/title
# Keep nav title for folders
key = python:item.get('_type') == 'Folder' and '_defaultpage' or 'title'
# Keep nav title for links
condition = python:not item.get('remoteUrl')\
# Use /head/title for everything else
    and '_tree' in item and item['_tree'].xpath("${title:xpath}")
value-python = u' '.join(element.text_content().strip() for element in\
                         item['_tree'].xpath("${title:xpath}"))
value = python:${title:value-python}

[description]
blueprint = collective.transmogrifier.sections.inserter
key = string:description
xpath = /html/head/meta[@name='description']/@content
condition = python:'_tree' in item
value = python:u' '.join(element.strip() for element in\
    item['_tree'].xpath("${description:xpath}"))

[subject]
blueprint = collective.transmogrifier.sections.inserter
key = string:subject
xpath = /html/head/meta[@name='keywords']/@content
condition = python:'_tree' in item and item['_tree'].xpath("${subject:xpath}")
value = python:u','.join(element.strip() for element in \
    item['_tree'].xpath("${subject:xpath}")).split(',')

[text]
blueprint = collective.transmogrifier.sections.inserter
key = string:text
condition = python:item.get('_type') != 'Folder' and '_content' in item
value = python:u'\n'.join([modules['lxml.etree'].tostring(\
        element, method='html', encoding=unicode, pretty_print=True)\
    for element in item['_content']])


## Free/remove memory intensive XML tree references
[cleanup-trees]
blueprint = collective.transmogrifier.sections.inserter
key = string:_has_response
condition = python:item.update(('_has' + key, bool(item.pop(key))) for key in (\
    '_response', '_tree', '_nav', '_content') if key in item)
value = nothing


## Get content data that doesn't require XML trees

[id]
blueprint = plone.app.transmogrifier.urlnormalizer
locale = string:en

[path]
blueprint = collective.transmogrifier.sections.inserter
key = string:_path
path = (('_content_element' in item\
         and modules['urllib'].unquote(item['_url'].path))\
    or modules['posixpath'].join(\
        item.get('_parent_path', modules['posixpath'].dirname(\
                 '_url' in item\
                 and modules['urllib'].unquote(item['_url'].path)\
                 or item.get('remoteUrl', item.get('_path', '${root:path}')))),\
# Also removes _id                
        item.pop('_id', modules['posixpath'].basename(\
                 '_url' in item\
                 and modules['urllib'].unquote(item['_url'].path)\
                 or item.get('remoteUrl', item.get('_path')))))\
# Append an extension to create duplicated nav objects with different titles
    + (('remoteUrl' in item and '_is_defaultpage' not in item\
        and '.url') or ''))
# Only register the URL as seen for pages, not folders,
#   since default pages will have the same URL
condition = python:'_path' not in item and '_url' in item\
    and (item.setdefault('_path', ${path:path})\
         and item.get('_type') != 'Folder' and\
        transmogrifier.__annotations__.setdefault(\
            'xmlwalker.paths', {}).setdefault(modules['urlparse'].urlunsplit(\
            item['_url']), item['_path']) or True)\
    and item.pop('_parent', True)
value = item/_path

[type]
blueprint = collective.transmogrifier.sections.inserter
key = string:_type
# How much of the body to classify
size = 2048
findTypeName = transmogrifier.context.content_type_registry.findTypeName(\
# Use the content path for the *.url extension if we're a Link
    item.get('remoteUrl') and item.get('_path')\
# Otherwise use the extension of the original URL or assume *.html
        or getattr(item.get('_url'), 'path', '_.html'),\
    item.get('_headers', {}).get('content-type', ''),\
# Read only some of the file for classification
    'file' in item and item['file'].read(${type:size}) or '')
condition = python:not item.get('_type') and ${type:findTypeName}
value = python:${type:findTypeName}

[modificationDate]
blueprint = collective.transmogrifier.sections.inserter
key = string:modificationDate
condition = python:'_headers' in item and item['_headers'].get('last-modified')
value = python:item['_headers']['last-modified']

[defaultpage]
blueprint = plone.app.transmogrifier.urlnormalizer
source-key = _defaultpage
destination-key = string:_defaultpage
locale = string:en

[folders]
blueprint = collective.transmogrifier.sections.folders

[excludeFromNav]
blueprint = collective.transmogrifier.sections.inserter
key = string:excludeFromNav
# Exclude all folders not listed in the navbar
condition = python:item.get('_type') == 'Folder'\
    and ('_has_element' not in item or '_content_element' in item)
value = python:True


## Add the content

[drop-construct]
blueprint = collective.transmogrifier.sections.manipulator
# Do not add content for the portal itself
condition = python:item.get('_path') == '/'
delete = _type

[log-content]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = python:item.get('_path') and item.get('_type')
delete =
    _cache
    _has_parent
    _has_element
    _has_defaultpage
    _has_response
    _has_tree
    _has_nav
    _has_content
    _headers
    _href
    _init
    _url
    file
    text

[construct]
blueprint = collective.transmogrifier.sections.constructor

[savepoint]
blueprint = collective.transmogrifier.sections.savepoint


## Debugging tools

[debug]
blueprint = collective.transmogrifier.sections.logger
# Change to True to log full items for debugging
condition = python:False
level = INFO
delete = text

[breaker]
blueprint = collective.transmogrifier.sections.breakpoint
# Change to True to log full items for debugging
condition = python:False
